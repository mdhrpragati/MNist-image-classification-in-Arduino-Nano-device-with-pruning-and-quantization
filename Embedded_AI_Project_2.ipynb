{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_yFf6DN2UVUg"
   },
   "source": [
    "# *IT00CH92 Embedded AI - Spring 2024*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFdPvlXBOdUN"
   },
   "source": [
    "# Training MNIST using TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bjmi3qZeu_xk"
   },
   "source": [
    "## Overview\n",
    "\n",
    "To effectively deploy deep neural models on resource-constrained edge devices, two critical aspects of model optimization are most useful: pruning, which involves reducing the size of a neural network by eliminating unnecessary parameters, and quantization, which involves reducing the precision of numerical values in the model to conserve memory and computational resources.\n",
    "\n",
    "For using these techniques, we define the application area by employing the popular LeNet-5 architecture trained on the classic MNIST dataset. LeNet-5, designed by Yann LeCun et al., represents one of the pioneering convolutional neural network (CNN) architectures and remains a benchmark for image classification tasks. At the end of the notebook, we obtain the trained full-precision model to be used for the further steps of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yEAZYXvZU_XG"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yJwIonXEVJo6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariamaoliveira/anaconda3/envs/embedded_ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "tf.random.set_seed(\n",
    "    seed=81\n",
    ")\n",
    "\n",
    "import tempfile\n",
    "from tensorflow_model_optimization.python.core.keras.compat import keras\n",
    "import tensorflow_model_optimization as tfmot\n",
    "# from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "psViY5PRDurp"
   },
   "source": [
    "## Using Tensorflow Datasets API to load MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3wmSQZ-tC-Df"
   },
   "source": [
    "TFDS datasets often come with data already split into different sets. For MNIST, it has splits for train and test. We use the [Slicing API](https://www.tensorflow.org/datasets/splits#slicing_api) for TFDS to create a validation split.\n",
    "\n",
    "Next, we aim to understand how the dataset is formatted and utilize visualizations. Finally, the dataset is preprocessed before being passed to the model. For preprocessing, we simply normalize the image values as float32 within the range [0, 1] for all three splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, info = tfds.load('mnist', with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "a7217ee4cab547f5bd35e08dc5f0bb5c",
      "abbbabf618b340ca98c4b623f5582792",
      "036928fe21ff4d7381aa8d7c02f39089",
      "e36546f6b8ea4d69a8b70a51153447c7",
      "8645cf9cdddd4d72bebbbb0da0005f3d",
      "94ce1a37e8a6457788337f1f48c7e2fb",
      "5844b2244607427d92350e7c0327ccba",
      "66ef62feaa4f4738b011dc3973f08e25",
      "1e60b011eeae4df8801b8303d7f79cc6",
      "d460425ff77b42c781d2fd475574541b",
      "278df0c98469440fa669af97e1065f90"
     ]
    },
    "id": "D2qa8aaB3y5i",
    "outputId": "2dc84f2e-39fa-478d-e984-a2197ce2b851"
   },
   "outputs": [],
   "source": [
    "(ds_train, ds_val, ds_test), ds_info = tfds.load(\n",
    "    'mnist',\n",
    "    split=['train[:90%]', 'train[90%:]', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "id": "2yd5TjPC9zRK",
    "outputId": "66331ff3-8f92-419a-f633-d105c9555190"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input image shape: (28, 28, 1)\n",
      "Image type: <dtype: 'uint8'>\n",
      "Label shape: ()\n",
      "Label type: <dtype: 'int64'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-20 17:31:57.998153: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-20 17:31:58.069726: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "ds = ds_train.take(1)  # Only take a single example\n",
    "for example in ds:  # example is `('image': tf.Tensor, 'label': tf.Tensor)`\n",
    "  image = example[0]\n",
    "  label = example[1]\n",
    "  print(f\"Input image shape: {image.shape}\\nImage type: {image.dtype}\")\n",
    "  print(f\"Label shape: {label.shape}\\nLabel type: {label.dtype}\")\n",
    "  plt.imshow(image, cmap='gray')\n",
    "  plt.title(label.numpy())\n",
    "  plt.axis(\"off\")\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 749
    },
    "id": "JjTQ7Ruv5roK",
    "outputId": "251789b8-8722-41d5-fbbe-118a00102ffe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-20 14:25:09.913693: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2024-05-20 14:25:09.926189: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x900 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = tfds.show_examples(ds_test, ds_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sMPLTPUMHVv2"
   },
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "sbrdXWdb44HR"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "n_epochs = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vb81aTYGHgOH"
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "hsE6J4wn4ZfK"
   },
   "outputs": [],
   "source": [
    "def normalize_img(image:tf.uint8, label:tf.int64):\n",
    "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "  return tf.cast(image, tf.float32) / 255., label\n",
    "\n",
    "def normalize_splits(ds, split_name: str, batch_size: int):\n",
    "  \"\"\"Applies preprocessing to train, val and test sets\"\"\"\n",
    "  ds = ds.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.AUTOTUNE\n",
    "  )\n",
    "  ds = ds.cache() # Caching makes it faster for consecutive runs\n",
    "  if split_name != 'test':\n",
    "    # Shuffling is not done for the test set\n",
    "    ds = ds.shuffle(ds_info.splits[split_name].num_examples)\n",
    "  ds = ds.batch(batch_size)\n",
    "  ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "UhW9GjoGHmWu"
   },
   "outputs": [],
   "source": [
    "ds_train = normalize_splits(ds_train, split_name='train[:90%]', batch_size=batch_size)\n",
    "ds_val = normalize_splits(ds_val, split_name='train[90%:]', batch_size=batch_size)\n",
    "ds_test = normalize_splits(ds_test, split_name='test', batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# tf.data.Dataset.from_tensor_slices((ds_train)).batch(1).take(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12avdMPHLYV_"
   },
   "source": [
    "## Training Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pbY-KGMPvbW9",
    "outputId": "624f3558-9ba1-4c78-e075-e75b860e515e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_6 (Conv2D)           (None, 28, 28, 6)         156       \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPoolin  (None, 14, 14, 6)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 14, 14, 16)        2416      \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPoolin  (None, 7, 7, 16)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 120)               94200     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 84)                10164     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 10)                850       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 107786 (421.04 KB)\n",
      "Trainable params: 107786 (421.04 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model architecture (LeNet-5).\n",
    "\n",
    "model = keras.Sequential([\n",
    "  keras.layers.InputLayer(input_shape=(28, 28, 1)),\n",
    "  keras.layers.Conv2D(filters=6, kernel_size=(5, 5), padding='same', activation='relu'),\n",
    "  keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "  keras.layers.Conv2D(filters=16, kernel_size=(5, 5), padding='same', activation='relu'),\n",
    "  keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "  keras.layers.Flatten(),\n",
    "  keras.layers.Dense(120),\n",
    "  keras.layers.Dense(84),\n",
    "  keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# As truth labels are integer encoded, we use sparse categorical cross-entropy as loss fn\n",
    "model.compile(optimizer='adam',\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False), # the ouputs are expected as probabilities\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-BG8So727vqp",
    "outputId": "977e812f-e9b0-4b9c-e0d5-0cb69c1b1f80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "422/422 [==============================] - 11s 8ms/step - loss: 0.2383 - accuracy: 0.9295 - val_loss: 0.0843 - val_accuracy: 0.9742\n",
      "Epoch 2/4\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 0.0711 - accuracy: 0.9786 - val_loss: 0.0665 - val_accuracy: 0.9805\n",
      "Epoch 3/4\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 0.0546 - accuracy: 0.9832 - val_loss: 0.0730 - val_accuracy: 0.9775\n",
      "Epoch 4/4\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 0.0447 - accuracy: 0.9857 - val_loss: 0.0514 - val_accuracy: 0.9860\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7e455a1e6dd0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "  ds_train,\n",
    "  epochs=n_epochs,\n",
    "  validation_data=ds_val,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RyIKnbVZafIH",
    "outputId": "fec99abb-ea76-4471-d606-79e1b370db3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline test accuracy: 0.9829999804496765\n"
     ]
    }
   ],
   "source": [
    "_, baseline_accuracy = model.evaluate(\n",
    "    ds_test, verbose=0)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Igc0jGH6NSf_",
    "outputId": "f7071e0b-361e-44fa-a52c-c52516acda6c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-41-bb00b51b2b61>:2: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  keras.models.save_model(model, './Full_Precision_MNIST_TF.h5', include_optimizer=False)\n"
     ]
    }
   ],
   "source": [
    "# Saving the model in .h5 format\n",
    "keras.models.save_model(model, './Full_Precision_MNIST_TF.h5', include_optimizer=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0O5xuci-SonI"
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O2I7xmyMW5QY"
   },
   "source": [
    "In this notebook, we trained a LeNet-5 neural network on the MNIST dataset using TensorFlow (and various of its APIs). It's important to note the size of the trained model in terms of the number of trainable parameters and the baseline test accuracy for further comparisons during the pruning and quantization steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune pre-trained model with pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load MNIST dataset\n",
    "# (train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# # Normalize the input image so that each pixel value is between 0 and 1.\n",
    "# train_images = train_images / 255.0\n",
    "# test_images = test_images / 255.0\n",
    "\n",
    "\n",
    "# mnist = tf.keras.datasets.mnist\n",
    "# (train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# # Normalize the input image so that each pixel value is between 0 to 1.\n",
    "# train_images = train_images.astype(np.float32) / 255.0\n",
    "# test_images = test_images.astype(np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "keras_file = 'saved_model/Full_Precision_MNIST_TF.h5'\n",
    "model = keras.models.load_model(keras_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " prune_low_magnitude_conv2d  (None, 28, 28, 6)         308       \n",
      "  (PruneLowMagnitude)                                            \n",
      "                                                                 \n",
      " prune_low_magnitude_max_po  (None, 14, 14, 6)         1         \n",
      " oling2d (PruneLowMagnitude                                      \n",
      " )                                                               \n",
      "                                                                 \n",
      " prune_low_magnitude_conv2d  (None, 14, 14, 16)        4818      \n",
      " _1 (PruneLowMagnitude)                                          \n",
      "                                                                 \n",
      " prune_low_magnitude_max_po  (None, 7, 7, 16)          1         \n",
      " oling2d_1 (PruneLowMagnitu                                      \n",
      " de)                                                             \n",
      "                                                                 \n",
      " prune_low_magnitude_flatte  (None, 784)               1         \n",
      " n (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_dense   (None, 120)               188282    \n",
      " (PruneLowMagnitude)                                             \n",
      "                                                                 \n",
      " prune_low_magnitude_dense_  (None, 84)                20246     \n",
      " 1 (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_dense_  (None, 10)                1692      \n",
      " 2 (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 215349 (841.24 KB)\n",
      "Trainable params: 107786 (421.04 KB)\n",
      "Non-trainable params: 107563 (420.20 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "# Compute end step to finish pruning after 2 epochs.\n",
    "batch_size = 128\n",
    "epochs = 2\n",
    "validation_split = 0.1 # 10% of training set will be used for validation set. \n",
    "\n",
    "num_images = len(ds_train)\n",
    "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
    "\n",
    "# Define model for pruning.\n",
    "pruning_params = {\n",
    "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
    "                                                               final_sparsity=0.80,\n",
    "                                                               begin_step=0,\n",
    "                                                               end_step=end_step)\n",
    "}\n",
    "\n",
    "\n",
    "model_for_pruning = prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "# `prune_low_magnitude` requires a recompile.\n",
    "model_for_pruning.compile(optimizer='adam',\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_for_pruning.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate the model against baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariamaoliveira/anaconda3/envs/embedded_ai/lib/python3.10/site-packages/tf_keras/src/backend.py:5729: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6/422 [..............................] - ETA: 4s - loss: 0.0294 - accuracy: 0.9935   WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0113s vs `on_train_batch_end` time: 0.0287s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0113s vs `on_train_batch_end` time: 0.0287s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422/422 [==============================] - 12s 13ms/step - loss: 0.0398 - accuracy: 0.9876 - val_loss: 0.0505 - val_accuracy: 0.9835\n",
      "Epoch 2/2\n",
      "422/422 [==============================] - 5s 11ms/step - loss: 0.0318 - accuracy: 0.9901 - val_loss: 0.0479 - val_accuracy: 0.9862\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x30cd79120>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logdir = tempfile.mkdtemp()\n",
    "\n",
    "callbacks = [\n",
    "  tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "  tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
    "]\n",
    "\n",
    "model_for_pruning.fit(ds_train,\n",
    "                  batch_size=batch_size, epochs=epochs, validation_data=ds_val,\n",
    "                  callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline test accuracy: 0.9829999804496765\n",
      "Pruned test accuracy: 0.9887999892234802\n"
     ]
    }
   ],
   "source": [
    "_, model_for_pruning_accuracy = model_for_pruning.evaluate(\n",
    "   ds_test, verbose=0)\n",
    "\n",
    "print('Baseline test accuracy:', 0.9829999804496765) \n",
    "print('Pruned test accuracy:', model_for_pruning_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create 3x smaller models from pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s7/3gb_v_vn6ns01664c_jtg6hh0000gn/T/ipykernel_1768/2832786679.py:7: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pruned Keras model to: saved_model/pruned_model_1.h5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Do the pruning\n",
    "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
    "\n",
    "# _, pruned_keras_file = tempfile.mkstemp('.h5')\n",
    "\n",
    "pruned_keras_file = \"saved model/pruned_model_1.h5\"\n",
    "keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\n",
    "print('Saved pruned Keras model to:', pruned_keras_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/s7/3gb_v_vn6ns01664c_jtg6hh0000gn/T/tmpeldnjzxx/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/s7/3gb_v_vn6ns01664c_jtg6hh0000gn/T/tmpeldnjzxx/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pruned TFLite model to: saved_model/pruned_model_tflite_1.tflite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1716213379.869503 1430236 tf_tfl_flatbuffer_helpers.cc:390] Ignored output_format.\n",
      "W0000 00:00:1716213379.869523 1430236 tf_tfl_flatbuffer_helpers.cc:393] Ignored drop_control_dependency.\n",
      "2024-05-20 16:56:19.869848: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/s7/3gb_v_vn6ns01664c_jtg6hh0000gn/T/tmpeldnjzxx\n",
      "2024-05-20 16:56:19.871678: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-05-20 16:56:19.871694: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /var/folders/s7/3gb_v_vn6ns01664c_jtg6hh0000gn/T/tmpeldnjzxx\n",
      "2024-05-20 16:56:19.886369: I tensorflow/cc/saved_model/loader.cc:234] Restoring SavedModel bundle.\n",
      "2024-05-20 16:56:19.924965: I tensorflow/cc/saved_model/loader.cc:218] Running initialization op on SavedModel bundle at path: /var/folders/s7/3gb_v_vn6ns01664c_jtg6hh0000gn/T/tmpeldnjzxx\n",
      "2024-05-20 16:56:19.941853: I tensorflow/cc/saved_model/loader.cc:317] SavedModel load for tags { serve }; Status: success: OK. Took 72005 microseconds.\n"
     ]
    }
   ],
   "source": [
    "# # Converting to TensorFlowLite\n",
    "# converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
    "# pruned_tflite_model = converter.convert()\n",
    "\n",
    "# # _, pruned_tflite_file = tempfile.mkstemp('.tflite')\n",
    "# pruned_tflite_file = \"saved_model/pruned_model_tflite_1.tflite\"\n",
    "\n",
    "# with open(pruned_tflite_file, 'wb') as f:\n",
    "#   f.write(pruned_tflite_model)\n",
    "\n",
    "# print('Saved pruned TFLite model to:', pruned_tflite_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gzipped_model_size(file):\n",
    "  # Returns size of gzipped model, in bytes.\n",
    "  import os\n",
    "  import zipfile\n",
    "\n",
    "  # _, zipped_file = tempfile.mkstemp('.zip')\n",
    "  zipped_file = \"saved_model/pruned_model_zipped_1.zip\"\n",
    "  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "    f.write(file)\n",
    "\n",
    "  return os.path.getsize(zipped_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of gzipped baseline Keras model: 1200921.00 bytes\n",
      "Size of gzipped pruned Keras model: 131239.00 bytes\n",
      "Size of gzipped pruned TFlite model: 126999.00 bytes\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file)))\n",
    "print(\"Size of gzipped pruned Keras model: %.2f bytes\" % (get_gzipped_model_size(pruned_keras_file)))\n",
    "print(\"Size of gzipped pruned TFlite model: %.2f bytes\" % (get_gzipped_model_size(pruned_tflite_file)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a 10x smaller model from combining pruning and quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full integer quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "pruned_model = keras.models.load_model(\"saved_model/pruned_model_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representative_data_gen():\n",
    "  for image, label  in ds_train.take(100) :\n",
    "    # image = tf.expand_dims(image, 0) \n",
    "    yield [image]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/s7/3gb_v_vn6ns01664c_jtg6hh0000gn/T/tmpa3j93_38/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/s7/3gb_v_vn6ns01664c_jtg6hh0000gn/T/tmpa3j93_38/assets\n",
      "/Users/mariamaoliveira/anaconda3/envs/embedded_ai/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:964: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "W0000 00:00:1716216203.170832   17487 tf_tfl_flatbuffer_helpers.cc:390] Ignored output_format.\n",
      "W0000 00:00:1716216203.170840   17487 tf_tfl_flatbuffer_helpers.cc:393] Ignored drop_control_dependency.\n",
      "2024-05-20 17:43:23.170949: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/s7/3gb_v_vn6ns01664c_jtg6hh0000gn/T/tmpa3j93_38\n",
      "2024-05-20 17:43:23.171490: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-05-20 17:43:23.171495: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /var/folders/s7/3gb_v_vn6ns01664c_jtg6hh0000gn/T/tmpa3j93_38\n",
      "2024-05-20 17:43:23.176321: I tensorflow/cc/saved_model/loader.cc:234] Restoring SavedModel bundle.\n",
      "2024-05-20 17:43:23.189355: I tensorflow/cc/saved_model/loader.cc:218] Running initialization op on SavedModel bundle at path: /var/folders/s7/3gb_v_vn6ns01664c_jtg6hh0000gn/T/tmpa3j93_38\n",
      "2024-05-20 17:43:23.194632: I tensorflow/cc/saved_model/loader.cc:317] SavedModel load for tags { serve }; Status: success: OK. Took 23682 microseconds.\n",
      "2024-05-20 17:43:26.006499: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: UINT8, output_inference_type: UINT8\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "# Ensure that if any ops can't be quantized, the converter throws an error\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# Set the input and output tensors to uint8 (APIs added in r2.3)\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "\n",
    "tflite_model_quant = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved quantized and pruned TFLite model to: saved_model/pruned_model_tflite_quantized_1.tflite\n",
      "Size of gzipped baseline Keras model: 1200921.00 bytes\n",
      "Size of gzipped pruned and quantized TFlite model: 97094.00 bytes\n"
     ]
    }
   ],
   "source": [
    "# converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
    "# converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# converter.representative_dataset = representative_dataset #provide part of the dataset\n",
    "# # converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# # converter.inference_input_type = tf.int8  # or tf.uint8\n",
    "# # converter.inference_output_type = tf.int8  # or tf.uint8\n",
    "# quantized_and_pruned_tflite_model = converter.convert()\n",
    "\n",
    "_, quantized_and_pruned_tflite_file = tempfile.mkstemp('.tflite')\n",
    "\n",
    "quantized_and_pruned_tflite_file = \"saved_model/pruned_model_tflite_quantized_1.tflite\"\n",
    "\n",
    "with open(quantized_and_pruned_tflite_file, 'wb') as f:\n",
    "  f.write(tflite_model_quant)\n",
    "\n",
    "print('Saved quantized and pruned TFLite model to:', quantized_and_pruned_tflite_file)\n",
    "\n",
    "print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file)))\n",
    "print(\"Size of gzipped pruned and quantized TFlite model: %.2f bytes\" % (get_gzipped_model_size(quantized_and_pruned_tflite_file)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See persistence of accuracy from TF to TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate_model(interpreter):\n",
    "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "  # Run predictions on ever y image in the \"test\" dataset.\n",
    "  prediction_digits = []\n",
    "  for i, test_image in enumerate(test_images):\n",
    "    if i % 1000 == 0:\n",
    "      print('Evaluated on {n} results so far.'.format(n=i))\n",
    "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
    "    # the model's input data format.\n",
    "    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
    "    test_image = np.expand_dims(test_image, axis=-1).astype(np.float32)\n",
    "    # print(test_image.shape)\n",
    "    # print(test_image)\n",
    "    interpreter.set_tensor(input_index, test_image)\n",
    "\n",
    "    # Run inference.\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Post-processing: remove batch dimension and find the digit with highest\n",
    "    # probability.\n",
    "    output = interpreter.tensor(output_index)\n",
    "    digit = np.argmax(output()[0])\n",
    "    prediction_digits.append(digit)\n",
    "\n",
    "  print('\\n')\n",
    "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "  prediction_digits = np.array(prediction_digits)\n",
    "  accuracy = (prediction_digits == test_labels).mean()\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=quantized_and_pruned_tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "test_accuracy = evaluate_model(interpreter)\n",
    "\n",
    "print('Pruned and quantized TFLite test_accuracy:', test_accuracy)\n",
    "print('Pruned TF test accuracy:', model_for_pruning_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the model to .h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header file, model.h, is 698,052 bytes.\n",
      "\n",
      "Open the side panel (refresh if needed). Double click model.h to download the file.\n"
     ]
    }
   ],
   "source": [
    "!echo \"const unsigned char model[] = {\" > classifying_imu/content/model.h\n",
    "!cat saved_model/pruned_model_tflite_quantized_1.tflite | xxd -i      >> classifying_imu/content/model.h\n",
    "!echo \"};\"                              >> classifying_imu/content/model.h\n",
    "\n",
    "import os\n",
    "model_h_size = os.path.getsize(\"classifying_imu/content/model.h\")\n",
    "print(f\"Header file, model.h, is {model_h_size:,} bytes.\")\n",
    "print(\"\\nOpen the side panel (refresh if needed). Double click model.h to download the file.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "036928fe21ff4d7381aa8d7c02f39089": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_66ef62feaa4f4738b011dc3973f08e25",
      "max": 5,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1e60b011eeae4df8801b8303d7f79cc6",
      "value": 5
     }
    },
    "1e60b011eeae4df8801b8303d7f79cc6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "278df0c98469440fa669af97e1065f90": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5844b2244607427d92350e7c0327ccba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "66ef62feaa4f4738b011dc3973f08e25": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8645cf9cdddd4d72bebbbb0da0005f3d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "94ce1a37e8a6457788337f1f48c7e2fb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a7217ee4cab547f5bd35e08dc5f0bb5c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_abbbabf618b340ca98c4b623f5582792",
       "IPY_MODEL_036928fe21ff4d7381aa8d7c02f39089",
       "IPY_MODEL_e36546f6b8ea4d69a8b70a51153447c7"
      ],
      "layout": "IPY_MODEL_8645cf9cdddd4d72bebbbb0da0005f3d"
     }
    },
    "abbbabf618b340ca98c4b623f5582792": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_94ce1a37e8a6457788337f1f48c7e2fb",
      "placeholder": "",
      "style": "IPY_MODEL_5844b2244607427d92350e7c0327ccba",
      "value": "DlCompleted...:100%"
     }
    },
    "d460425ff77b42c781d2fd475574541b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e36546f6b8ea4d69a8b70a51153447c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d460425ff77b42c781d2fd475574541b",
      "placeholder": "",
      "style": "IPY_MODEL_278df0c98469440fa669af97e1065f90",
      "value": "5/5[00:00&lt;00:00,13.41file/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
